{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-Artificial-Neural-Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aplneto/IF1014/blob/main/07_Artificial_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I3lh2vm0AE5"
      },
      "source": [
        "# <center> Missão 7 </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN4aTqH-0Pl5"
      },
      "source": [
        "#Equipe:\n",
        "Antônio Paulino - apln2@cin.ufpe.br\n",
        "Ailton Rodrigues - ajr@cin.ufpe.br\n",
        "Douglas Tavares - dtrps@cin.ufpe.br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGMHgQOozqSf"
      },
      "source": [
        "#Parte 1: As redes neurais artificiais estão mais do que nunca em alta devido aos seus benefícios em inúmeras áreas e pelo crescente volume de dados. Em um problema a ser tratado com métodos de aprendizagem de máquina, seria melhor optar por modelos já vistos na disciplina ou partir diretamente para modelos de redes neurais? por que?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKLqBRK0zsGb"
      },
      "source": [
        "\n",
        " \n",
        "Como um bom Cientista de Dados/Engenheiro de Aprendizado de Máquina, você deve estar bem ciente do arsenal que possui e qual arma deve se usada para garantir o resultado máximo.\n",
        " \n",
        "Partindo dessa premissa, existem 3 razões principais para partir diretamente para modelos de redes neurais:\n",
        " \n",
        "i.               Há disponibilização de muitos dados\n",
        " \n",
        "a.    As redes neurais exigem muito mais dados do que os algoritmos de aprendizado de máquina tradicionais para fazer seu trabalho bem; logo, se existem muitos dados disponíveis a opção é redes neurais.\n",
        " \n",
        "ii.             Poder computacional\n",
        " \n",
        "a.    As redes neurais exigem um processo de treinamento e para isso é necessário ter um poder computacional alto pra poder executar os experimentos. Como atualmente o poder computacional dobra a cada dois anos, então isso não será empecilho para não usar redes neurais.\n",
        " \n",
        "iii.            Algoritmo de retropropagação\n",
        " \n",
        "a.    A retropropagação ajuda na formação de redes neurais artificiais. Quando as redes neurais artificiais se formam, os valores dos pesos são atribuídos aleatoriamente. O utilizador estabelece pesos aleatórios porque não está ciente dos valores corretos. Quando o valor é diferente da rede de alimentação esperada, considere-o como um erro. O algoritmo é definido para que o modelo altere os parâmetros cada vez que a saída não é a esperada. O erro tem uma relação com redes neurais artificiais. Assim, quando o parâmetro muda, o erro também muda até a rede neural encontrar a saída desejada através do cálculo do gradiente de descida\n",
        " \n",
        " \n",
        "Ademais, possuem vantagens que ratificam o por quê das redes neurais estarem no auge e por serem uma “arma” predileta do arsenal:\n",
        " \n",
        "i.               a capacidade de aprender e modelar relações não lineares e complexas, o que é realmente importante porque, na vida real, muitas das relações entre entradas e saídas são não lineares e também complexas.  Ademais, o crescente volume de dados propicia o treinamento de modelos mais robustos com o de aprendizado profundo.\n",
        " \n",
        "ii.              As redes neurais podem generalizar - depois de aprender com as entradas iniciais e seus relacionamentos, pode inferir relacionamentos não vistos em dados não vistos também, fazendo com que o modelo generalize e preveja em dados não vistos.\n",
        " \n",
        "iii.             Ao contrário de muitas outras técnicas de predição, a RNA não impõe nenhuma restrição às variáveis de entrada (por exemplo, como devem ser distribuídas). Além disso, muitos estudos mostraram que as RNAs podem modelar melhor dados com alta volatilidade e variância não constante, dada sua capacidade de aprender relações ocultas nos dados sem impor quaisquer relações fixas nos dados. Isso é algo muito útil na previsão de séries temporais financeiras (por exemplo, preços de ações) em que a volatilidade dos dados é muito alta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux6ZZkB0rlk0"
      },
      "source": [
        "## Instalação dos pacotes necessários"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3naDSnrri29",
        "outputId": "92333ad9-0d3b-429c-a0ab-895cbc375c30"
      },
      "source": [
        "!python3 -m pip install optuna"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 184 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 235 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 256 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 276 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 296 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 308 kB 20.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.4-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.2)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.26)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.9.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.7.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 39.3 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.2.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=efca9fb1c2cab5263bf13a5b7a6a46c08e8181651908cc5ed18569158b61ed1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.5 alembic-1.7.4 autopage-0.4.0 cliff-3.9.0 cmaes-0.8.2 cmd2-2.2.0 colorama-0.4.4 colorlog-6.6.0 optuna-2.10.0 pbr-5.7.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmKNKLSLqz02"
      },
      "source": [
        "# Análise Exploratória e preparação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CGnpF9BdWSB"
      },
      "source": [
        "DATA_FOLDER = (\n",
        "    'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
        "    'credit-screening/'\n",
        ")\n",
        "\n",
        "DATA_DESCRIPTION = DATA_FOLDER + 'crx.names'\n",
        "DATA_SET = DATA_FOLDER + 'crx.data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Iz_TsY5Tdgbg",
        "outputId": "a2b4cd9f-6c91-4a94-9cbb-cfc3df4a6b04"
      },
      "source": [
        "import pandas\n",
        "import numpy\n",
        "\n",
        "aliases = [\n",
        "  'Gender', 'Age', 'Debt', 'Married', 'BankCustomer', 'EducationLevel',\n",
        "  'Ethnicity', 'YearsEmployed', 'PriorDefault', 'Employed', 'CreditScore',\n",
        "  'DriversLicense', 'Citizen', 'ZipCode', 'Income', 'Approved'\n",
        "]\n",
        "data = pandas.read_csv(DATA_SET, names=aliases, na_values='?', header=None)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Debt</th>\n",
              "      <th>Married</th>\n",
              "      <th>BankCustomer</th>\n",
              "      <th>EducationLevel</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>YearsEmployed</th>\n",
              "      <th>PriorDefault</th>\n",
              "      <th>Employed</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>DriversLicense</th>\n",
              "      <th>Citizen</th>\n",
              "      <th>ZipCode</th>\n",
              "      <th>Income</th>\n",
              "      <th>Approved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b</td>\n",
              "      <td>30.83</td>\n",
              "      <td>0.000</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.25</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>1</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>202.0</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>3.04</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>43.0</td>\n",
              "      <td>560</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>1.50</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>280.0</td>\n",
              "      <td>824</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.540</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>3.75</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>5</td>\n",
              "      <td>t</td>\n",
              "      <td>g</td>\n",
              "      <td>100.0</td>\n",
              "      <td>3</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b</td>\n",
              "      <td>20.17</td>\n",
              "      <td>5.625</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.71</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>s</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender    Age   Debt Married  ... Citizen ZipCode Income  Approved\n",
              "0      b  30.83  0.000       u  ...       g   202.0      0         +\n",
              "1      a  58.67  4.460       u  ...       g    43.0    560         +\n",
              "2      a  24.50  0.500       u  ...       g   280.0    824         +\n",
              "3      b  27.83  1.540       u  ...       g   100.0      3         +\n",
              "4      b  20.17  5.625       u  ...       s   120.0      0         +\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ivtaIwH_I2"
      },
      "source": [
        "#### Remoção das variáveis Ethnicity (A7) e ZipCode (A14) por não exercerem influência na variável alvo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "trYeSElbhziC",
        "outputId": "f6888baa-8d42-4983-da6f-ea68d841e6a2"
      },
      "source": [
        "# removing useless variables A7 (Ethnicity) and A14 (ZipCode)\n",
        "\n",
        "data.drop(['Ethnicity', 'ZipCode'], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Debt</th>\n",
              "      <th>Married</th>\n",
              "      <th>BankCustomer</th>\n",
              "      <th>EducationLevel</th>\n",
              "      <th>YearsEmployed</th>\n",
              "      <th>PriorDefault</th>\n",
              "      <th>Employed</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>DriversLicense</th>\n",
              "      <th>Citizen</th>\n",
              "      <th>Income</th>\n",
              "      <th>Approved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b</td>\n",
              "      <td>30.83</td>\n",
              "      <td>0.000</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>1.25</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>1</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>3.04</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>560</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>1.50</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>824</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.540</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>3.75</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>5</td>\n",
              "      <td>t</td>\n",
              "      <td>g</td>\n",
              "      <td>3</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b</td>\n",
              "      <td>20.17</td>\n",
              "      <td>5.625</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>1.71</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>s</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender    Age   Debt Married  ... DriversLicense Citizen  Income Approved\n",
              "0      b  30.83  0.000       u  ...              f       g       0        +\n",
              "1      a  58.67  4.460       u  ...              f       g     560        +\n",
              "2      a  24.50  0.500       u  ...              f       g     824        +\n",
              "3      b  27.83  1.540       u  ...              t       g       3        +\n",
              "4      b  20.17  5.625       u  ...              f       s       0        +\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5doQ0tMJLTS_"
      },
      "source": [
        "#### Separação das variáveis em contínuas e categóricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBJtk0g_jDgn",
        "outputId": "890d9dac-f896-4991-98a0-15f7b9105a97"
      },
      "source": [
        "continuous = data.describe().columns\n",
        "categorical = data.drop(list(continuous) + ['Approved'], axis=1).columns\n",
        "\n",
        "print(continuous)\n",
        "print(categorical)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Age', 'Debt', 'YearsEmployed', 'CreditScore', 'Income'], dtype='object')\n",
            "Index(['Gender', 'Married', 'BankCustomer', 'EducationLevel', 'PriorDefault',\n",
            "       'Employed', 'DriversLicense', 'Citizen'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9GhYwlTrA3q"
      },
      "source": [
        "# Limpeza dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH2FCGXmrON8"
      },
      "source": [
        "## Modelo de regressão linear para completar dados continuos ausentes\n",
        "\n",
        "Variáveis continuas ausentes serão preenchidas por valores previstos a partir de um modelo de regressão linear construído a partir da coluna com valores ausentes e da coluna com todos os valores mais fortemente correlacionada a ela"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yKlqfJ7qB8E",
        "outputId": "2d7a8884-d0ca-4d59-c052-de6897f192c2"
      },
      "source": [
        "continuous_columns_missing_values = []\n",
        "\n",
        "for column in continuous:\n",
        "  if data[column].isnull().sum() > 0:\n",
        "    continuous_columns_missing_values.append(column)\n",
        "\n",
        "print(continuous_columns_missing_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Age']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9C-K0POuJPu"
      },
      "source": [
        "most_correlated_columns = {}\n",
        "candidates = [\n",
        "  x for x in continuous if x not in continuous_columns_missing_values\n",
        "]\n",
        "for column in continuous_columns_missing_values:\n",
        "  most_correlated_columns[column] = max(\n",
        "      candidates, key=lambda x: abs(data[x].corr(data[column]))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_6R1Is2g_J"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XBOZPrjzyt8",
        "outputId": "cb750303-308f-4eea-deb5-317bce2ae78e"
      },
      "source": [
        "prediction_models = {}\n",
        "\n",
        "for pair in most_correlated_columns.items():\n",
        "  rows = data[~data[list(pair)].isnull().any(axis=1)][list(pair)]\n",
        "  y = rows[pair[0]]\n",
        "  x = rows[pair[1]]\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x.values.reshape(-1, 1), y)\n",
        "  d = pandas.DataFrame(data= {\n",
        "      'value' : lr.predict(data[pair[1]].values.reshape(-1, 1))\n",
        "  })\n",
        "  data[pair[0]] = numpy.where(data[column].isna(), d['value'], data[column])\n",
        "\n",
        "data[continuous].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Age              0\n",
              "Debt             0\n",
              "YearsEmployed    0\n",
              "CreditScore      0\n",
              "Income           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnGULe_0dydD"
      },
      "source": [
        "# Codificação ortogonal\n",
        "\n",
        "Valores categóricos foram codificados no formato one hot encoding.\n",
        "\n",
        "Os valores ausentes foram completados usando um algoritmo de árvore de decisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkUzbEYp_F_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97313ba-9b2f-48f7-e47a-3bde87f79958"
      },
      "source": [
        "categorical_columns_missing_values = [\n",
        "  p[0] for p in dict(data[categorical].isna().sum() > 0).items() if p[1]\n",
        "]\n",
        "complete_data = data.dropna()\n",
        "print(categorical_columns_missing_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Gender', 'Married', 'BankCustomer', 'EducationLevel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZU1dCFeM8J"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "\n",
        "label_dict = defaultdict(LabelEncoder)\n",
        "complete_data = complete_data.apply(\n",
        "    lambda x: label_dict[x.name].fit_transform(x)\n",
        "    if x.name in list(categorical) + ['Approved']\n",
        "    else x\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHORVpp3nBO3"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLfuGF1hno0q"
      },
      "source": [
        "trees = {}\n",
        "X = complete_data.drop(categorical_columns_missing_values, axis=1)\n",
        "for column in categorical_columns_missing_values:\n",
        "  Y = complete_data[column]\n",
        "  tree = DecisionTreeClassifier(\n",
        "      max_leaf_nodes=Y.nunique(), random_state=2**Y.nunique()\n",
        "  )\n",
        "  trees[column] = tree\n",
        "  tree.fit(X.values, Y.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBZ1FL4Xo7MH"
      },
      "source": [
        "for column in trees:\n",
        "  tree = trees[column]\n",
        "  encoder = label_dict[column]\n",
        "  d = pandas.DataFrame(data= {\n",
        "      'value' : encoder.inverse_transform(\n",
        "          tree.predict(\n",
        "          data.drop(categorical_columns_missing_values, axis=1).apply(\n",
        "                  lambda x: label_dict[x.name].fit_transform(x)\n",
        "                  if x.name in list(categorical) + ['Approved']\n",
        "                  else x\n",
        "          ).values\n",
        "        )\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "\n",
        "  data[column] = numpy.where(data[column].isna(), d['value'], data[column])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1LG6PLI2J7B"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhWKHKZH2U1s"
      },
      "source": [
        "onehotencoders = defaultdict(OneHotEncoder)\n",
        "\n",
        "new_categories = []\n",
        "\n",
        "for column in categorical:\n",
        "  encoder = onehotencoders[column]\n",
        "  encoder.fit(data[column].values.reshape(-1, 1))\n",
        "  arr = encoder.transform(data[column].values.reshape(-1, 1)).toarray()\n",
        "  data.drop(column, axis=1, inplace=True)\n",
        "  d = dict(zip(*[column+'_'+ c for c in encoder.categories_], zip(*arr)))\n",
        "  for k in d:\n",
        "    data[k] = d[k]\n",
        "    new_categories.append(k)\n",
        "\n",
        "data.head()\n",
        "categorical = new_categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_d2Y2lm2PjP"
      },
      "source": [
        "# encoder = OneHotEncoder()\n",
        "# encoder.fit(data['Gender'].values.reshape(-1, 1))\n",
        "# arr = encoder.transform(data['Gender'].values.reshape(-1, 1)).toarray()\n",
        "# d = dict(zip(*encoder.categories_, zip(*arr)))\n",
        "# t = dict(zip(*['data_'+ c for c in encoder.categories_], zip(*arr)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNZ8AWwLtXHn",
        "outputId": "0d59ebbf-8a99-40da-ed19-29ead963ca57"
      },
      "source": [
        "labels = data['Approved']\n",
        "data.drop('Approved', axis=1, inplace=True)\n",
        "X = data.apply(\n",
        "    lambda x: label_dict[x.name].fit_transform(x)\n",
        "    if x.name in categorical\n",
        "    else x\n",
        ")\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Age    Debt  YearsEmployed  ...  Citizen_g  Citizen_p  Citizen_s\n",
            "0    30.83   0.000           1.25  ...          1          0          0\n",
            "1    58.67   4.460           3.04  ...          1          0          0\n",
            "2    24.50   0.500           1.50  ...          1          0          0\n",
            "3    27.83   1.540           3.75  ...          1          0          0\n",
            "4    20.17   5.625           1.71  ...          0          0          1\n",
            "..     ...     ...            ...  ...        ...        ...        ...\n",
            "685  21.08  10.085           1.25  ...          1          0          0\n",
            "686  22.67   0.750           2.00  ...          1          0          0\n",
            "687  25.25  13.500           2.00  ...          1          0          0\n",
            "688  17.92   0.205           0.04  ...          1          0          0\n",
            "689  35.00   3.375           8.29  ...          1          0          0\n",
            "\n",
            "[690 rows x 36 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvxT_R-xuGMx",
        "outputId": "5962dc93-bf8c-4207-a108-55f0ab691ed6"
      },
      "source": [
        "Y = pandas.DataFrame(\n",
        "    LabelEncoder().fit_transform(labels), columns=[labels.name]\n",
        ")\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Approved\n",
            "0           0\n",
            "1           0\n",
            "2           0\n",
            "3           0\n",
            "4           0\n",
            "..        ...\n",
            "685         1\n",
            "686         1\n",
            "687         1\n",
            "688         1\n",
            "689         1\n",
            "\n",
            "[690 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcreK8mFTCxT"
      },
      "source": [
        "# Divisão das instâncias em treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHQCdNHTeoP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakIeVNxTErW"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X.values, Y.values, test_size = 0.2, random_state = 4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh-jFCdYjLTl"
      },
      "source": [
        "X_train = pandas.DataFrame(X_train, columns=X.columns)\n",
        "X_test = pandas.DataFrame(X_test, columns=X.columns)\n",
        "Y_train = pandas.DataFrame(Y_train, columns=Y.columns)\n",
        "Y_test = pandas.DataFrame(Y_test, columns=Y.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z5Bz26JUptq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538f54e2-aa2d-4875-a011-a5523440d9c2"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(552, 36)\n",
            "(552, 1)\n",
            "(138, 36)\n",
            "(138, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOyEMSbgjYSv"
      },
      "source": [
        "# Transformação dos Dados\n",
        "\n",
        "* Uma vez que as variáveis continuas possuem valores entre 0 e um determinado limite, estes serão normalizados entre os valores 0.0 e 1.0 para análise de diminuição de dimensionalidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTvmhyKvzcZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9a94d796-f28c-448e-f481-2a5775eca73e"
      },
      "source": [
        "X_train[continuous].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Debt</th>\n",
              "      <th>YearsEmployed</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>552.000000</td>\n",
              "      <td>552.000000</td>\n",
              "      <td>552.000000</td>\n",
              "      <td>552.000000</td>\n",
              "      <td>552.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>31.375927</td>\n",
              "      <td>4.723342</td>\n",
              "      <td>2.222554</td>\n",
              "      <td>2.574275</td>\n",
              "      <td>1016.204710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>11.791370</td>\n",
              "      <td>4.958010</td>\n",
              "      <td>3.342170</td>\n",
              "      <td>5.163208</td>\n",
              "      <td>5328.577631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>13.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>22.500000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.165000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>28.448036</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.520000</td>\n",
              "      <td>7.312500</td>\n",
              "      <td>2.551250</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>462.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>80.250000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>28.500000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Age        Debt  YearsEmployed  CreditScore         Income\n",
              "count  552.000000  552.000000     552.000000   552.000000     552.000000\n",
              "mean    31.375927    4.723342       2.222554     2.574275    1016.204710\n",
              "std     11.791370    4.958010       3.342170     5.163208    5328.577631\n",
              "min     13.750000    0.000000       0.000000     0.000000       0.000000\n",
              "25%     22.500000    0.875000       0.165000     0.000000       0.000000\n",
              "50%     28.448036    2.750000       1.000000     0.000000       5.500000\n",
              "75%     37.520000    7.312500       2.551250     3.000000     462.250000\n",
              "max     80.250000   28.000000      28.500000    67.000000  100000.000000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "7lhbVr-WipyM",
        "outputId": "bd1d5366-e84c-40bf-d735-b1c29aa6b348"
      },
      "source": [
        "X_test[continuous].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Debt</th>\n",
              "      <th>YearsEmployed</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>138.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>138.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>32.437283</td>\n",
              "      <td>4.900254</td>\n",
              "      <td>2.226812</td>\n",
              "      <td>1.702899</td>\n",
              "      <td>1022.108696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12.172970</td>\n",
              "      <td>5.073769</td>\n",
              "      <td>3.376051</td>\n",
              "      <td>3.331796</td>\n",
              "      <td>4724.580194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.170000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>23.080000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>29.585000</td>\n",
              "      <td>2.730000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>39.107500</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>76.750000</td>\n",
              "      <td>25.210000</td>\n",
              "      <td>17.500000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Age        Debt  YearsEmployed  CreditScore        Income\n",
              "count  138.000000  138.000000     138.000000   138.000000    138.000000\n",
              "mean    32.437283    4.900254       2.226812     1.702899   1022.108696\n",
              "std     12.172970    5.073769       3.376051     3.331796   4724.580194\n",
              "min     15.170000    0.000000       0.000000     0.000000      0.000000\n",
              "25%     23.080000    1.250000       0.125000     0.000000      0.000000\n",
              "50%     29.585000    2.730000       0.750000     0.000000      1.000000\n",
              "75%     39.107500    7.000000       2.750000     2.000000    200.000000\n",
              "max     76.750000   25.210000      17.500000    20.000000  50000.000000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFRyOvlpuP4X"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLJPUx6qugEz"
      },
      "source": [
        "train_scalers = defaultdict(MinMaxScaler)\n",
        "test_scalers = defaultdict(MinMaxScaler)\n",
        "\n",
        "for column in continuous:\n",
        "  train_scaler = train_scalers[column]\n",
        "  test_scaler = test_scalers[column]\n",
        "  X_train[column] = train_scaler.fit_transform(X_train[column].values.reshape(-1, 1))\n",
        "  X_test[column] = test_scaler.fit_transform(X_test[column].values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKfx-vqR4vqN"
      },
      "source": [
        "# Redução da dimensionalidade\n",
        "\n",
        "<!--\n",
        "\n",
        "* https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/\n",
        "\n",
        " -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij0GvkjSHjVO"
      },
      "source": [
        "## Principal component analysis\n",
        "\n",
        "<!--\n",
        "\n",
        "* https://www.datasklr.com/principal-component-analysis-and-factor-analysis/principal-component-analysis\n",
        "* https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
        "* https://jmausolf.github.io/code/pca_in_python/\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYX9jgmx_OE"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqVnMY7hE6cj",
        "outputId": "70d8ce0d-57a4-4042-ee51-569057c85688"
      },
      "source": [
        "pca_train = PCA()\n",
        "pca_train.fit(X_train[continuous])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tgXduEantyP",
        "outputId": "fd74fc34-da51-4934-a452-6363d012ec4e"
      },
      "source": [
        "numpy.cumsum(pca_train.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.49681538, 0.7953898 , 0.91164728, 0.96771056, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "WLDtFu5P89M1",
        "outputId": "24cd5147-191b-4548-aa18-3b44b012398c"
      },
      "source": [
        "pca_X_train = pandas.DataFrame(\n",
        "    data = pca_train.transform(X_train[continuous]),\n",
        "    columns = ['PC%d' % (i) for i in numpy.arange(pca_train.n_components_)+1]\n",
        ")\n",
        "\n",
        "max_column = numpy.argmax(numpy.cumsum(pca_train.explained_variance_ratio_) > 0.9) + 1\n",
        "principal_components = pca_X_train.columns[:max_column:]\n",
        "\n",
        "pca_X_train = pandas.concat(\n",
        "    [pca_X_train[principal_components], X_train[categorical]],\n",
        "    axis = 1\n",
        ")\n",
        "\n",
        "pca_X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>Gender_a</th>\n",
              "      <th>Gender_b</th>\n",
              "      <th>Married_l</th>\n",
              "      <th>Married_u</th>\n",
              "      <th>Married_y</th>\n",
              "      <th>BankCustomer_g</th>\n",
              "      <th>BankCustomer_gg</th>\n",
              "      <th>BankCustomer_p</th>\n",
              "      <th>EducationLevel_aa</th>\n",
              "      <th>EducationLevel_c</th>\n",
              "      <th>EducationLevel_cc</th>\n",
              "      <th>EducationLevel_d</th>\n",
              "      <th>EducationLevel_e</th>\n",
              "      <th>EducationLevel_ff</th>\n",
              "      <th>EducationLevel_i</th>\n",
              "      <th>EducationLevel_j</th>\n",
              "      <th>EducationLevel_k</th>\n",
              "      <th>EducationLevel_m</th>\n",
              "      <th>EducationLevel_q</th>\n",
              "      <th>EducationLevel_r</th>\n",
              "      <th>EducationLevel_w</th>\n",
              "      <th>EducationLevel_x</th>\n",
              "      <th>PriorDefault_f</th>\n",
              "      <th>PriorDefault_t</th>\n",
              "      <th>Employed_f</th>\n",
              "      <th>Employed_t</th>\n",
              "      <th>DriversLicense_f</th>\n",
              "      <th>DriversLicense_t</th>\n",
              "      <th>Citizen_g</th>\n",
              "      <th>Citizen_p</th>\n",
              "      <th>Citizen_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.080758</td>\n",
              "      <td>-0.184270</td>\n",
              "      <td>-0.065782</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.216770</td>\n",
              "      <td>-0.018099</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.189481</td>\n",
              "      <td>-0.012675</td>\n",
              "      <td>0.048359</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.087857</td>\n",
              "      <td>-0.047823</td>\n",
              "      <td>-0.124984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.130168</td>\n",
              "      <td>0.191575</td>\n",
              "      <td>-0.026154</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>-0.143830</td>\n",
              "      <td>0.060022</td>\n",
              "      <td>0.087901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>0.041393</td>\n",
              "      <td>0.023916</td>\n",
              "      <td>0.093961</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>-0.167171</td>\n",
              "      <td>-0.040376</td>\n",
              "      <td>-0.002445</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>0.028834</td>\n",
              "      <td>0.238163</td>\n",
              "      <td>-0.045819</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>0.125133</td>\n",
              "      <td>0.276131</td>\n",
              "      <td>-0.029625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>552 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          PC1       PC2       PC3  ...  Citizen_g  Citizen_p  Citizen_s\n",
              "0   -0.080758 -0.184270 -0.065782  ...        0.0        1.0        0.0\n",
              "1   -0.216770 -0.018099  0.009423  ...        0.0        0.0        1.0\n",
              "2   -0.189481 -0.012675  0.048359  ...        1.0        0.0        0.0\n",
              "3    0.087857 -0.047823 -0.124984  ...        1.0        0.0        0.0\n",
              "4   -0.130168  0.191575 -0.026154  ...        1.0        0.0        0.0\n",
              "..        ...       ...       ...  ...        ...        ...        ...\n",
              "547 -0.143830  0.060022  0.087901  ...        1.0        0.0        0.0\n",
              "548  0.041393  0.023916  0.093961  ...        0.0        0.0        1.0\n",
              "549 -0.167171 -0.040376 -0.002445  ...        1.0        0.0        0.0\n",
              "550  0.028834  0.238163 -0.045819  ...        1.0        0.0        0.0\n",
              "551  0.125133  0.276131 -0.029625  ...        1.0        0.0        0.0\n",
              "\n",
              "[552 rows x 34 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "cg-zatkDoRz9",
        "outputId": "2aa3523a-4967-4ffe-a502-cda7846066d4"
      },
      "source": [
        "pca_X_test = pandas.DataFrame(\n",
        "    data = pca_train.transform(X_test[continuous]),\n",
        "    columns = ['PC%d' % (i) for i in numpy.arange(pca_train.n_components_)+1]\n",
        ")\n",
        "\n",
        "principal_components = pca_X_test.columns[:max_column:]\n",
        "\n",
        "pca_X_test = pandas.concat(\n",
        "    [pca_X_test[principal_components], X_test[categorical]],\n",
        "    axis = 1\n",
        ")\n",
        "\n",
        "pca_X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>Gender_a</th>\n",
              "      <th>Gender_b</th>\n",
              "      <th>Married_l</th>\n",
              "      <th>Married_u</th>\n",
              "      <th>Married_y</th>\n",
              "      <th>BankCustomer_g</th>\n",
              "      <th>BankCustomer_gg</th>\n",
              "      <th>BankCustomer_p</th>\n",
              "      <th>EducationLevel_aa</th>\n",
              "      <th>EducationLevel_c</th>\n",
              "      <th>EducationLevel_cc</th>\n",
              "      <th>EducationLevel_d</th>\n",
              "      <th>EducationLevel_e</th>\n",
              "      <th>EducationLevel_ff</th>\n",
              "      <th>EducationLevel_i</th>\n",
              "      <th>EducationLevel_j</th>\n",
              "      <th>EducationLevel_k</th>\n",
              "      <th>EducationLevel_m</th>\n",
              "      <th>EducationLevel_q</th>\n",
              "      <th>EducationLevel_r</th>\n",
              "      <th>EducationLevel_w</th>\n",
              "      <th>EducationLevel_x</th>\n",
              "      <th>PriorDefault_f</th>\n",
              "      <th>PriorDefault_t</th>\n",
              "      <th>Employed_f</th>\n",
              "      <th>Employed_t</th>\n",
              "      <th>DriversLicense_f</th>\n",
              "      <th>DriversLicense_t</th>\n",
              "      <th>Citizen_g</th>\n",
              "      <th>Citizen_p</th>\n",
              "      <th>Citizen_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013034</td>\n",
              "      <td>0.154900</td>\n",
              "      <td>0.158099</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.616798</td>\n",
              "      <td>-0.115678</td>\n",
              "      <td>-0.053012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.141278</td>\n",
              "      <td>0.023545</td>\n",
              "      <td>-0.017648</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.207157</td>\n",
              "      <td>-0.161917</td>\n",
              "      <td>0.414889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.293477</td>\n",
              "      <td>0.043979</td>\n",
              "      <td>0.033666</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>-0.131200</td>\n",
              "      <td>0.060932</td>\n",
              "      <td>0.003077</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>-0.245238</td>\n",
              "      <td>0.012432</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.305105</td>\n",
              "      <td>-0.170936</td>\n",
              "      <td>-0.040540</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0.237959</td>\n",
              "      <td>0.276361</td>\n",
              "      <td>-0.153109</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>-0.108243</td>\n",
              "      <td>0.153372</td>\n",
              "      <td>-0.005270</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>138 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          PC1       PC2       PC3  ...  Citizen_g  Citizen_p  Citizen_s\n",
              "0    0.013034  0.154900  0.158099  ...        1.0        0.0        0.0\n",
              "1    0.616798 -0.115678 -0.053012  ...        1.0        0.0        0.0\n",
              "2   -0.141278  0.023545 -0.017648  ...        1.0        0.0        0.0\n",
              "3    0.207157 -0.161917  0.414889  ...        0.0        0.0        1.0\n",
              "4   -0.293477  0.043979  0.033666  ...        1.0        0.0        0.0\n",
              "..        ...       ...       ...  ...        ...        ...        ...\n",
              "133 -0.131200  0.060932  0.003077  ...        1.0        0.0        0.0\n",
              "134 -0.245238  0.012432  0.017578  ...        1.0        0.0        0.0\n",
              "135  0.305105 -0.170936 -0.040540  ...        1.0        0.0        0.0\n",
              "136  0.237959  0.276361 -0.153109  ...        1.0        0.0        0.0\n",
              "137 -0.108243  0.153372 -0.005270  ...        1.0        0.0        0.0\n",
              "\n",
              "[138 rows x 34 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pYuZsRApWII"
      },
      "source": [
        "import optuna\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsHDlzzoC7D7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e321b323-aca4-4506-b5e2-a1ecac6ebc2c"
      },
      "source": [
        "(1,None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, None)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnyu_n_2ruyU"
      },
      "source": [
        "def model_factory(hidden_layers, activation_fn, _solver, l2_alpha, **args):\n",
        "  return MLPClassifier(\n",
        "      hidden_layer_sizes=hidden_layers,\n",
        "      activation = activation_fn,\n",
        "      solver = _solver,\n",
        "      alpha = l2_alpha,\n",
        "      **args\n",
        "  )\n",
        "\n",
        "def mlp_model_optimization_study(trial: optuna.trial.FixedTrial):\n",
        "  insize = pca_X_train.values.shape[1] + 1\n",
        "  outsize = 2\n",
        "  number_of_layers = trial.suggest_int('number_of_layers', 1, 2)\n",
        "  hl1 = trial.suggest_int('hidden_layer_1', insize*2, 100)\n",
        "  hidden_layers = (\n",
        "      (hl1, trial.suggest_int('hidden_layer_2', outsize, insize))\n",
        "      if number_of_layers > 1\n",
        "      else (hl1,)\n",
        "  )\n",
        "  activation_fn = trial.suggest_categorical(\n",
        "      'activation_fn', ['identity', 'logistic', 'tanh', 'relu']\n",
        "  )\n",
        "  _solver = trial.suggest_categorical(\n",
        "      '_solver', ['lbfgs', 'sgd', 'adam']\n",
        "  )\n",
        "  l2_alpha = trial.suggest_loguniform('l2_alpha', 0.1, 10)\n",
        "  early_stopping = (\n",
        "      trial.suggest_categorical('early_stopping', [False, True])\n",
        "      if _solver != 'lbfgs'\n",
        "      else False\n",
        "  )\n",
        "  epochs = (\n",
        "      trial.suggest_int('epochs', 100, 500, 50)\n",
        "      if _solver != 'lbfgs'\n",
        "      else trial.suggest_int('epochs', 5000, 10000, 1000)\n",
        "  )\n",
        "  model = (\n",
        "      model_factory(hidden_layers, activation_fn, _solver, l2_alpha,\n",
        "                    early_stopping = True)\n",
        "      if early_stopping\n",
        "      else model_factory(hidden_layers, activation_fn, _solver, l2_alpha,\n",
        "                         early_stopping = True,\n",
        "                         max_iter=epochs)\n",
        "  )\n",
        "  model.fit(pca_X_train.values, Y_train.values.ravel())\n",
        "  trial.set_user_attr(\n",
        "      'training_score', model.score(pca_X_train.values, Y_train.values.ravel())\n",
        "  )\n",
        "  return model.score(pca_X_test.values, Y_test.values.ravel())\n",
        "  \n",
        "#adam e sgd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN_DVEpOur4S",
        "outputId": "ad642a8a-99b0-4c39-f0e1-001211df8b43"
      },
      "source": [
        "study = optuna.study.create_study(\n",
        "    study_name='MLP_study', direction='maximize'\n",
        ")\n",
        "study.optimize(mlp_model_optimization_study, n_trials=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-11-09 23:40:09,040]\u001b[0m A new study created in memory with name: MLP_study\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:09,138]\u001b[0m Trial 0 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 88, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 2.361587584583161, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:09,451]\u001b[0m Trial 1 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 94, 'hidden_layer_2': 28, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.38623342815442774, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:11,644]\u001b[0m Trial 2 finished with value: 0.782608695652174 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 72, 'hidden_layer_2': 4, 'activation_fn': 'tanh', '_solver': 'lbfgs', 'l2_alpha': 2.672146512157626, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:11,726]\u001b[0m Trial 3 finished with value: 0.5362318840579711 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 98, 'activation_fn': 'logistic', '_solver': 'sgd', 'l2_alpha': 0.6378021003502743, 'early_stopping': True, 'epochs': 150}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:11,808]\u001b[0m Trial 4 finished with value: 0.5797101449275363 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 74, 'hidden_layer_2': 25, 'activation_fn': 'logistic', '_solver': 'sgd', 'l2_alpha': 0.1212691478295516, 'early_stopping': True, 'epochs': 500}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:11,920]\u001b[0m Trial 5 finished with value: 0.7536231884057971 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 99, 'hidden_layer_2': 15, 'activation_fn': 'relu', '_solver': 'sgd', 'l2_alpha': 0.2669782639577277, 'early_stopping': True, 'epochs': 450}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:13,278]\u001b[0m Trial 6 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 100, 'activation_fn': 'tanh', '_solver': 'lbfgs', 'l2_alpha': 1.0408192345210885, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:13,358]\u001b[0m Trial 7 finished with value: 0.7898550724637681 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 87, 'hidden_layer_2': 3, 'activation_fn': 'identity', '_solver': 'adam', 'l2_alpha': 0.2211923163971529, 'early_stopping': True, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,129]\u001b[0m Trial 8 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 90, 'activation_fn': 'tanh', '_solver': 'lbfgs', 'l2_alpha': 1.8290751679763582, 'epochs': 6000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,224]\u001b[0m Trial 9 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 87, 'hidden_layer_2': 27, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.12601945165074255, 'early_stopping': False, 'epochs': 350}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,396]\u001b[0m Trial 10 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 8.909630451297666, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,497]\u001b[0m Trial 11 finished with value: 0.782608695652174 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 9.905753876856924, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,660]\u001b[0m Trial 12 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 8.275827280261568, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,808]\u001b[0m Trial 13 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 81, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 4.616511546392484, 'early_stopping': False, 'epochs': 200}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:14,980]\u001b[0m Trial 14 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 75, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 3.984881205351323, 'early_stopping': False, 'epochs': 300}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,079]\u001b[0m Trial 15 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 84, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.8641373892441826, 'early_stopping': False, 'epochs': 300}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,164]\u001b[0m Trial 16 finished with value: 0.7608695652173914 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 91, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 5.350085853060257, 'early_stopping': False, 'epochs': 200}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,343]\u001b[0m Trial 17 finished with value: 0.7753623188405797 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'logistic', '_solver': 'adam', 'l2_alpha': 1.1009424164446038, 'early_stopping': False, 'epochs': 200}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,491]\u001b[0m Trial 18 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 70, 'activation_fn': 'identity', '_solver': 'adam', 'l2_alpha': 2.4828658764260316, 'early_stopping': False, 'epochs': 300}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,761]\u001b[0m Trial 19 finished with value: 0.7681159420289855 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 83, 'activation_fn': 'relu', '_solver': 'sgd', 'l2_alpha': 6.435457945977476, 'early_stopping': False, 'epochs': 350}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,887]\u001b[0m Trial 20 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 94, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.6023502767462247, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:15,999]\u001b[0m Trial 21 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 3.808398255966452, 'early_stopping': False, 'epochs': 300}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:16,143]\u001b[0m Trial 22 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 75, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 3.2034408062833295, 'early_stopping': False, 'epochs': 350}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:16,506]\u001b[0m Trial 23 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 95, 'hidden_layer_2': 34, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.4277779054602619, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:16,595]\u001b[0m Trial 24 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 87, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 6.651996069167257, 'early_stopping': False, 'epochs': 250}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:16,878]\u001b[0m Trial 25 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 92, 'hidden_layer_2': 16, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.3132088682398522, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:17,177]\u001b[0m Trial 26 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 95, 'hidden_layer_2': 35, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.5906879988316582, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:17,412]\u001b[0m Trial 27 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 89, 'hidden_layer_2': 34, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 1.399545675014654, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:17,709]\u001b[0m Trial 28 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 92, 'hidden_layer_2': 16, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.1870658809392279, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:17,944]\u001b[0m Trial 29 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 97, 'hidden_layer_2': 9, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.7179268937571383, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:18,040]\u001b[0m Trial 30 finished with value: 0.42028985507246375 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 82, 'hidden_layer_2': 22, 'activation_fn': 'logistic', '_solver': 'sgd', 'l2_alpha': 0.4386100396873699, 'early_stopping': True, 'epochs': 150}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:18,411]\u001b[0m Trial 31 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 95, 'hidden_layer_2': 29, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.4037340539101279, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:18,656]\u001b[0m Trial 32 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 97, 'hidden_layer_2': 9, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.712940358724965, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:27,980]\u001b[0m Trial 33 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 97, 'hidden_layer_2': 10, 'activation_fn': 'tanh', '_solver': 'lbfgs', 'l2_alpha': 0.7843868512202417, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:28,188]\u001b[0m Trial 34 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 94, 'hidden_layer_2': 9, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 1.449499961189383, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:28,544]\u001b[0m Trial 35 finished with value: 0.7608695652173914 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 85, 'hidden_layer_2': 21, 'activation_fn': 'tanh', '_solver': 'sgd', 'l2_alpha': 2.517977314547982, 'early_stopping': True, 'epochs': 100}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:28,865]\u001b[0m Trial 36 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 89, 'hidden_layer_2': 15, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.33142684252902016, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:29,310]\u001b[0m Trial 37 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 100, 'hidden_layer_2': 30, 'activation_fn': 'logistic', '_solver': 'lbfgs', 'l2_alpha': 0.8819624547178171, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:29,444]\u001b[0m Trial 38 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 73, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 4.040552906185216, 'early_stopping': False, 'epochs': 300}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:29,655]\u001b[0m Trial 39 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 89, 'hidden_layer_2': 20, 'activation_fn': 'tanh', '_solver': 'sgd', 'l2_alpha': 0.28993988862694386, 'early_stopping': False, 'epochs': 150}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:30,057]\u001b[0m Trial 40 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 95, 'hidden_layer_2': 34, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.52910562970223, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:30,355]\u001b[0m Trial 41 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 86, 'hidden_layer_2': 13, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.20162780494781926, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:30,640]\u001b[0m Trial 42 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 98, 'hidden_layer_2': 6, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.35433800245169733, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:30,907]\u001b[0m Trial 43 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 89, 'hidden_layer_2': 12, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.49702543850966907, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:31,188]\u001b[0m Trial 44 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 86, 'hidden_layer_2': 13, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.17682266279853864, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:31,460]\u001b[0m Trial 45 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 99, 'hidden_layer_2': 6, 'activation_fn': 'identity', '_solver': 'lbfgs', 'l2_alpha': 0.13806093210335993, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:32,927]\u001b[0m Trial 46 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 92, 'activation_fn': 'logistic', '_solver': 'lbfgs', 'l2_alpha': 0.5161624177203399, 'epochs': 5000}. Best is trial 0 with value: 0.8333333333333334.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,080]\u001b[0m Trial 47 finished with value: 0.8405797101449275 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 79, 'hidden_layer_2': 6, 'activation_fn': 'identity', '_solver': 'adam', 'l2_alpha': 0.10702772892258801, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,238]\u001b[0m Trial 48 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.10069207657105315, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,334]\u001b[0m Trial 49 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 84, 'activation_fn': 'identity', '_solver': 'adam', 'l2_alpha': 0.16624521727038533, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,507]\u001b[0m Trial 50 finished with value: 0.8405797101449275 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 81, 'hidden_layer_2': 32, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.24213816348863587, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,662]\u001b[0m Trial 51 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 77, 'hidden_layer_2': 6, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.25619669676854573, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,824]\u001b[0m Trial 52 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.10037518652086674, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:33,956]\u001b[0m Trial 53 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.1079361056733725, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,087]\u001b[0m Trial 54 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 81, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.23299106245559884, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,227]\u001b[0m Trial 55 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.14055285078857574, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,362]\u001b[0m Trial 56 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.1436208909045702, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,464]\u001b[0m Trial 57 finished with value: 0.7898550724637681 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 75, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.12214027098387056, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,585]\u001b[0m Trial 58 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 81, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.14721933932969772, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,711]\u001b[0m Trial 59 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 83, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 9.491890370934081, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,835]\u001b[0m Trial 60 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 81, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.14679425626774695, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:34,941]\u001b[0m Trial 61 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.11975621327335204, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,064]\u001b[0m Trial 62 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.10862266177642296, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,183]\u001b[0m Trial 63 finished with value: 0.782608695652174 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 76, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.15831923690942837, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,279]\u001b[0m Trial 64 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 82, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.2080030993819816, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,435]\u001b[0m Trial 65 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.904239842987169, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,564]\u001b[0m Trial 66 finished with value: 0.8405797101449275 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.12922530864203285, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,728]\u001b[0m Trial 67 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.12787829941313789, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,864]\u001b[0m Trial 68 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 76, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 6.382285829983187, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:35,951]\u001b[0m Trial 69 finished with value: 0.7536231884057971 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 82, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.2380645264741292, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,086]\u001b[0m Trial 70 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 73, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 3.040614337051236, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,200]\u001b[0m Trial 71 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.11215402630429912, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,315]\u001b[0m Trial 72 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 81, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.18562930430471672, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,485]\u001b[0m Trial 73 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 70, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 2.9587091050606165, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,617]\u001b[0m Trial 74 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 73, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 0.1345875840558569, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,699]\u001b[0m Trial 75 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 76, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.16034893047553647, 'early_stopping': False, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:36,929]\u001b[0m Trial 76 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 72, 'activation_fn': 'tanh', '_solver': 'sgd', 'l2_alpha': 0.11473770871390329, 'early_stopping': True, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,034]\u001b[0m Trial 77 finished with value: 0.782608695652174 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 76, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.16193753090034352, 'early_stopping': False, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,173]\u001b[0m Trial 78 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 84, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 0.19372772500473154, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,265]\u001b[0m Trial 79 finished with value: 0.5797101449275363 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 83, 'activation_fn': 'logistic', '_solver': 'adam', 'l2_alpha': 1.195256382575032, 'early_stopping': False, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,405]\u001b[0m Trial 80 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 74, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 4.61932732664993, 'early_stopping': False, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,544]\u001b[0m Trial 81 finished with value: 0.8405797101449275 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.6819568022448246, 'early_stopping': False, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,744]\u001b[0m Trial 82 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 2.158074362529835, 'early_stopping': True, 'epochs': 200}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,835]\u001b[0m Trial 83 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 74, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 3.359784054035189, 'early_stopping': False, 'epochs': 250}. Best is trial 47 with value: 0.8405797101449275.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:37,947]\u001b[0m Trial 84 finished with value: 0.8478260869565217 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.78204935313095, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,050]\u001b[0m Trial 85 finished with value: 0.7753623188405797 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.1725074852667956, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,121]\u001b[0m Trial 86 finished with value: 0.41304347826086957 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'sgd', 'l2_alpha': 1.3723315079695457, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,258]\u001b[0m Trial 87 finished with value: 0.8333333333333334 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.7021782913888894, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,353]\u001b[0m Trial 88 finished with value: 0.5434782608695652 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 71, 'activation_fn': 'logistic', '_solver': 'adam', 'l2_alpha': 2.7602519889809645, 'early_stopping': False, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,478]\u001b[0m Trial 89 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.6918520971197308, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,608]\u001b[0m Trial 90 finished with value: 0.8115942028985508 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 75, 'activation_fn': 'tanh', '_solver': 'adam', 'l2_alpha': 1.7149624714130376, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,736]\u001b[0m Trial 91 finished with value: 0.8405797101449275 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.14035295714643797, 'early_stopping': False, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:38,846]\u001b[0m Trial 92 finished with value: 0.8043478260869565 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 76, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 2.0140892159885158, 'early_stopping': False, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,060]\u001b[0m Trial 93 finished with value: 0.8188405797101449 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 88, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.9387847842067578, 'early_stopping': False, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,163]\u001b[0m Trial 94 finished with value: 0.7971014492753623 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 80, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 2.3527980744859684, 'early_stopping': True, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,257]\u001b[0m Trial 95 finished with value: 0.7608695652173914 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 79, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 1.60563249608761, 'early_stopping': False, 'epochs': 150}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,432]\u001b[0m Trial 96 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 78, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.16839519991138527, 'early_stopping': False, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,578]\u001b[0m Trial 97 finished with value: 0.8260869565217391 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 82, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 0.10134589683755206, 'early_stopping': False, 'epochs': 150}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,716]\u001b[0m Trial 98 finished with value: 0.7898550724637681 and parameters: {'number_of_layers': 1, 'hidden_layer_1': 77, 'activation_fn': 'relu', '_solver': 'adam', 'l2_alpha': 2.145380835972524, 'early_stopping': False, 'epochs': 250}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n",
            "\u001b[32m[I 2021-11-09 23:40:39,806]\u001b[0m Trial 99 finished with value: 0.5797101449275363 and parameters: {'number_of_layers': 2, 'hidden_layer_1': 80, 'hidden_layer_2': 24, 'activation_fn': 'relu', '_solver': 'sgd', 'l2_alpha': 0.14275867462000857, 'early_stopping': True, 'epochs': 200}. Best is trial 84 with value: 0.8478260869565217.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwvD94DvBBhw",
        "outputId": "0d2af2f9-decc-473e-cf83-1502fdd181c0"
      },
      "source": [
        "for i, trial in enumerate(study.trials):\n",
        "  print('Trial %i' % i)\n",
        "  print('Acurácia de treinamento:',trial.user_attrs['training_score'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0\n",
            "Acurácia de treinamento: 0.8713768115942029\n",
            "Trial 1\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 2\n",
            "Acurácia de treinamento: 0.9402173913043478\n",
            "Trial 3\n",
            "Acurácia de treinamento: 0.5489130434782609\n",
            "Trial 4\n",
            "Acurácia de treinamento: 0.5489130434782609\n",
            "Trial 5\n",
            "Acurácia de treinamento: 0.802536231884058\n",
            "Trial 6\n",
            "Acurácia de treinamento: 0.9329710144927537\n",
            "Trial 7\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 8\n",
            "Acurácia de treinamento: 0.9166666666666666\n",
            "Trial 9\n",
            "Acurácia de treinamento: 0.8641304347826086\n",
            "Trial 10\n",
            "Acurácia de treinamento: 0.875\n",
            "Trial 11\n",
            "Acurácia de treinamento: 0.8460144927536232\n",
            "Trial 12\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 13\n",
            "Acurácia de treinamento: 0.8786231884057971\n",
            "Trial 14\n",
            "Acurácia de treinamento: 0.8786231884057971\n",
            "Trial 15\n",
            "Acurácia de treinamento: 0.8586956521739131\n",
            "Trial 16\n",
            "Acurácia de treinamento: 0.8206521739130435\n",
            "Trial 17\n",
            "Acurácia de treinamento: 0.7916666666666666\n",
            "Trial 18\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 19\n",
            "Acurácia de treinamento: 0.8315217391304348\n",
            "Trial 20\n",
            "Acurácia de treinamento: 0.875\n",
            "Trial 21\n",
            "Acurácia de treinamento: 0.875\n",
            "Trial 22\n",
            "Acurácia de treinamento: 0.8804347826086957\n",
            "Trial 23\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 24\n",
            "Acurácia de treinamento: 0.855072463768116\n",
            "Trial 25\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 26\n",
            "Acurácia de treinamento: 0.8840579710144928\n",
            "Trial 27\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 28\n",
            "Acurácia de treinamento: 0.8876811594202898\n",
            "Trial 29\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 30\n",
            "Acurácia de treinamento: 0.45108695652173914\n",
            "Trial 31\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 32\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 33\n",
            "Acurácia de treinamento: 0.9855072463768116\n",
            "Trial 34\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 35\n",
            "Acurácia de treinamento: 0.8315217391304348\n",
            "Trial 36\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 37\n",
            "Acurácia de treinamento: 0.8913043478260869\n",
            "Trial 38\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 39\n",
            "Acurácia de treinamento: 0.8460144927536232\n",
            "Trial 40\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 41\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 42\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 43\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 44\n",
            "Acurácia de treinamento: 0.8858695652173914\n",
            "Trial 45\n",
            "Acurácia de treinamento: 0.8876811594202898\n",
            "Trial 46\n",
            "Acurácia de treinamento: 0.9094202898550725\n",
            "Trial 47\n",
            "Acurácia de treinamento: 0.8713768115942029\n",
            "Trial 48\n",
            "Acurácia de treinamento: 0.8913043478260869\n",
            "Trial 49\n",
            "Acurácia de treinamento: 0.8695652173913043\n",
            "Trial 50\n",
            "Acurácia de treinamento: 0.8822463768115942\n",
            "Trial 51\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 52\n",
            "Acurácia de treinamento: 0.8840579710144928\n",
            "Trial 53\n",
            "Acurácia de treinamento: 0.8822463768115942\n",
            "Trial 54\n",
            "Acurácia de treinamento: 0.8804347826086957\n",
            "Trial 55\n",
            "Acurácia de treinamento: 0.8967391304347826\n",
            "Trial 56\n",
            "Acurácia de treinamento: 0.8931159420289855\n",
            "Trial 57\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 58\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 59\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 60\n",
            "Acurácia de treinamento: 0.8677536231884058\n",
            "Trial 61\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 62\n",
            "Acurácia de treinamento: 0.8713768115942029\n",
            "Trial 63\n",
            "Acurácia de treinamento: 0.8659420289855072\n",
            "Trial 64\n",
            "Acurácia de treinamento: 0.8405797101449275\n",
            "Trial 65\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 66\n",
            "Acurácia de treinamento: 0.8913043478260869\n",
            "Trial 67\n",
            "Acurácia de treinamento: 0.8822463768115942\n",
            "Trial 68\n",
            "Acurácia de treinamento: 0.8677536231884058\n",
            "Trial 69\n",
            "Acurácia de treinamento: 0.8405797101449275\n",
            "Trial 70\n",
            "Acurácia de treinamento: 0.8605072463768116\n",
            "Trial 71\n",
            "Acurácia de treinamento: 0.8804347826086957\n",
            "Trial 72\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 73\n",
            "Acurácia de treinamento: 0.875\n",
            "Trial 74\n",
            "Acurácia de treinamento: 0.8641304347826086\n",
            "Trial 75\n",
            "Acurácia de treinamento: 0.8242753623188406\n",
            "Trial 76\n",
            "Acurácia de treinamento: 0.8460144927536232\n",
            "Trial 77\n",
            "Acurácia de treinamento: 0.8514492753623188\n",
            "Trial 78\n",
            "Acurácia de treinamento: 0.8840579710144928\n",
            "Trial 79\n",
            "Acurácia de treinamento: 0.5489130434782609\n",
            "Trial 80\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 81\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 82\n",
            "Acurácia de treinamento: 0.8786231884057971\n",
            "Trial 83\n",
            "Acurácia de treinamento: 0.8677536231884058\n",
            "Trial 84\n",
            "Acurácia de treinamento: 0.8713768115942029\n",
            "Trial 85\n",
            "Acurácia de treinamento: 0.8586956521739131\n",
            "Trial 86\n",
            "Acurácia de treinamento: 0.447463768115942\n",
            "Trial 87\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 88\n",
            "Acurácia de treinamento: 0.5960144927536232\n",
            "Trial 89\n",
            "Acurácia de treinamento: 0.8840579710144928\n",
            "Trial 90\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 91\n",
            "Acurácia de treinamento: 0.875\n",
            "Trial 92\n",
            "Acurácia de treinamento: 0.8623188405797102\n",
            "Trial 93\n",
            "Acurácia de treinamento: 0.8822463768115942\n",
            "Trial 94\n",
            "Acurácia de treinamento: 0.8586956521739131\n",
            "Trial 95\n",
            "Acurácia de treinamento: 0.8260869565217391\n",
            "Trial 96\n",
            "Acurácia de treinamento: 0.8822463768115942\n",
            "Trial 97\n",
            "Acurácia de treinamento: 0.8731884057971014\n",
            "Trial 98\n",
            "Acurácia de treinamento: 0.8768115942028986\n",
            "Trial 99\n",
            "Acurácia de treinamento: 0.5489130434782609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTagTMY0mSb"
      },
      "source": [
        "# <center>Considerações Finais </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ITwHA00teg"
      },
      "source": [
        "  O algoritmo de redes neurais foi treinado com 4 funções de ativação: *identity*, *logistic*, *tanh* e *relu*. Como também 3 *solvers*: lbfgs, sgd, adam. Foi utilizado também a regulariação nas redes neurais para prevenir o *overfitting*, porém no *Scikit learn* só tinha disponível a regularização L2, em pesquisas realizadas a regularização L2 trabalha melhor no intervalo de 0.1 à 10, foi por este motivo que utilizamos a variação no parametros destes valores. Também foi utilizado o *early stop* que é outra forma usada para evitar o *overfitting*. É muito utilizado para verificar se o modelo de aprendizado continua aprendendo a cada epoch, caso a acurácia de treinamento caía o treinamento é interropido para que não haja uma piora nos resultados, pois caso o algoritmo continuasse poderia haver o *overfitting*.\n",
        "Para os *solvers* sgd e adam foram usados a variação das epoch de 100 à 500, com incremento de 50. Já para o *solver* lbfgs foi utilizado a variação de 1000 à 5000, com incremento de 1000. Para o número de camadas foram testados dois valores 1 e 2, já para as camadas escondidas variando de 1 até 100.\n",
        "\n",
        "\n",
        "Diante dessa configuração, 100 combinações foram avaliadas, sendo a melhor combinação a de número 84 composta com: Número de camadas = 1, camadas escondidas_1 = 79, função de ativação: relu, solver: adam, L2 = 1,78 e número de epochs = 250, neste caso não foi necessário utilizar o Early stopping. Essa combinação obteve uma acurácia de 0,84 para   treinamento e 0,87 para teste.\n",
        "\n",
        "\n",
        "Observa-se que a acurácia de treinamento e de teste tem um diferença de apenas 3 pontos percentuais, apesar disso, conclui-se que o modelo treinado é adequado e não sofreu *underfitting* e nem *overfitting* sendo útil para tarefa de aprovação de crédito conforme experimentação realizada. Observa-se também que se o algoritmo for treinado com estes parâmetros é possível que ocorra uma melhora da acurácia.\n",
        "\n",
        "O algoritmo redes neurais apresentou ser um algoritmo poderoso em relação aos outros modelos analisados, pois obteve a acurácia de 0,87 na fase de teste; enquanto os algoritmos SVR, K-NN e LVQ (com os valores ótimos para o conjunto de teste) - experimentados em missões anteriores -  obtiveram acurácia de 0,83, 0,78 e 0,62 respectivamente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RFB1Ul2zn6k"
      },
      "source": [
        "## <center> Referências </center>\n",
        "\n",
        "* https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
        "\n",
        "\n",
        "* https://medium.datadriveninvestor.com/when-not-to-use-neural-networks-89fb50622429\n",
        "\n",
        "* https://datascience.eu/pt/inteligencia-artificial/como-funciona-o-algoritmo-de-retropropagacao/\n",
        "\n",
        "* https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html\n",
        "\n",
        "* https://www.deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/\n",
        "\n",
        "* https://medium.com/turing-talks/turing-talks-20-regress%C3%A3o-de-ridge-e-lasso-a0fc467b5629\n",
        "\n",
        "* https://medium.com/data-hackers/o-que-%C3%A9-regulariza%C3%A7%C3%A3o-caa1967b8b13\n"
      ]
    }
  ]
}